{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "CompositionalSemantics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeWildering/comp-semantics/blob/main/CompositionalSemantics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz60OPnswvEl"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuUP7XRVwvEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86802848-86ba-411b-dd11-43d10dd13801"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "\n",
        "# Get the interactive Tools for Matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "# !python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "#Show all columns of dataframe\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = None"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jHiX33P7sNV"
      },
      "source": [
        "Load word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zkw5kIZwvEo",
        "outputId": "819ce335-31ea-4772-c867-530977231535"
      },
      "source": [
        "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "if not os.path.exists('/content/glove.6B.zip'):\n",
        "  urllib.request.urlretrieve('https://nlp.stanford.edu/data/glove.6B.zip', 'glove.6B.zip')\n",
        "if not os.path.exists('/content/glove.6B.100d.txt'):\n",
        "  !unzip /content/glove.6B #unzip compressed file\n",
        "\n",
        "glove_file = datapath('/content/glove.6B.300d.txt')\n",
        "\n",
        "tmp_file = \"/content/glove_tmp\"\n",
        "glove2word2vec(glove_file, tmp_file)\n",
        "print(tmp_file)\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "/content/glove_tmp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsy3JW5jwvEo"
      },
      "source": [
        "Gensim word similarity functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTLiNpZswvEp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "dbb367df-0615-43fc-a3b9-df31a81b6aa1"
      },
      "source": [
        "print(model.most_similar('kant'))\n",
        "print(model.most_similar(negative='opal'))\n",
        "print(model.similarity(\"happy\", \"unhappy\"))\n",
        "\n",
        "result = model.most_similar(positive=['colorful','sky'], negative=['colorless','uncolored','colourless'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))\n",
        "\n",
        "def analogy(x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result\n",
        "print(analogy('male', 'deer', 'female'))\n",
        "\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n",
        "\n",
        "def display_pca_scatterplot(model, words=None, sample=0):\n",
        "    if words == None:\n",
        "        if sample > 0:\n",
        "            words = np.random.choice(list(model.index2word), sample)\n",
        "        else:\n",
        "            words = [ word for word in model.vocab ]\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)\n",
        "\n",
        "display_pca_scatterplot(model, [\"woman\", \"man\", \"boss\", \"professional\", \"amateur\", \"president\",\n",
        "                                \"happy\", \"unhappy\", \"sad\", \"ecstatic\", \"displeased\", \"joyful\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('immanuel', 0.6443918943405151), ('hegel', 0.616576075553894), ('schopenhauer', 0.5629166960716248), ('nietzsche', 0.5363221168518066), ('spinoza', 0.5046462416648865), ('aristotle', 0.5012214183807373), ('plato', 0.5011799335479736), ('kantian', 0.500676155090332), ('metaphysics', 0.493084192276001), ('philosophers', 0.48889487981796265)]\n",
            "[('keyrates', 0.5349509119987488), ('rw97', 0.49975526332855225), ('23aou94', 0.4935912489891052), ('ryryryryryry', 0.4564247131347656), ('fazilah', 0.4550938606262207), ('27aou94', 0.45430704951286316), ('zety', 0.442155659198761), ('http://www.opel.com', 0.43997466564178467), ('+9.00', 0.43731439113616943), ('.70952', 0.43573370575904846)]\n",
            "0.5622518\n",
            "brought: 0.4219\n",
            "[('elk', 0.6696363687515259), ('boar', 0.5721302032470703), ('moose', 0.537537693977356), ('shmahn', 0.5307357311248779), ('antelope', 0.5266754627227783), ('white-tailed', 0.5219253301620483), ('squirrels', 0.5185477137565613), ('foxes', 0.5006103515625), ('rabbits', 0.49660614132881165), ('hunting', 0.48864394426345825)]\n",
            "cereal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4BiKdBAwvEz"
      },
      "source": [
        "Define functions for semantic composition (old)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVBUbE75zYXf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "a27a8bbe-f542-4eeb-821c-3bb0a07cc2b9"
      },
      "source": [
        "'''\n",
        "def get_features(text):\n",
        "  \"\"\"Extract linguistic features via Spacy. Also calculate dependency weights\"\"\"\n",
        "  doc = nlp(text)\n",
        "  sent = pd.DataFrame(columns=['token','dep','head','head_pos','children'])\n",
        "  for token in doc:\n",
        "    sent = sent.append({'token': token.text, 'pos': token.pos_, 'dep': token.dep_, 'head': token.head.text, 'head_pos': token.head.pos_,\n",
        "              'children': [child for child in token.children]}, ignore_index=True)\n",
        "  return sent\n",
        "\n",
        "# This one is not very good. Use child count instead.\n",
        "def add_dep_wts_head(features):\n",
        "  '''Add dependency weights based on number of tokens with given word listed as head.'''\n",
        "  counts = features['head'].value_counts()\n",
        "  dep_weight = []\n",
        "  for token in features['token']:\n",
        "    dep_weight.append(counts.get(token, 0) + 1)\n",
        "  features.insert(4, \"dep_wt\", dep_weight)\n",
        "  return features\n",
        "\n",
        "def add_dep_wts(features):\n",
        "  '''Add dependency weights based on number of child nodes.'''\n",
        "  features['dep_wt'] = features.apply(lambda row: len(row['children']) + 1, axis=1)\n",
        "  return features\n",
        "\n",
        "def add_dep_wts_log(features):\n",
        "  '''Add dependency weights based on number of child nodes.'''\n",
        "  features['dep_wt_log'] = features.apply(lambda row: math.log(row['dep_wt'], 10) + 1, axis=1)\n",
        "  return features\n",
        "\n",
        "def get_ants(token_df):\n",
        "  pos_keys = {'VERB': 'v',\n",
        "              'ADV': 'r',\n",
        "              'ADJ': 'a',\n",
        "              'NOUN': 'n',\n",
        "              }\n",
        "  ants = []\n",
        "  for w,p in zip(token_df.token, token_df.pos):\n",
        "    if p in pos_keys.keys():                      # only consider verb, adv, adj, and noun\n",
        "      for s in wn.synsets(w, pos=pos_keys[p]):\n",
        "        for l in s.lemmas():\n",
        "          if l.antonyms():\n",
        "            ants.append(l.antonyms()[0].name()) # get first antonym for any sense\n",
        "            break\n",
        "          else:\n",
        "            ants.append(None)\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "        break\n",
        "    else:\n",
        "      ants.append(None)\n",
        "  token_df.insert(len(token_df.columns), \"ant\", ants)\n",
        "  return token_df'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-498223a9ea1b>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    '''Add dependency weights based on number of tokens with given word listed as head.'''\u001b[0m\n\u001b[0m                                                                                          \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Amu50Lf8wjS"
      },
      "source": [
        "Scratch work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY0e40dr8vyM"
      },
      "source": [
        "def get_features(text):\n",
        "  \"\"\"Extract linguistic features via Spacy. Also calculate dependency weights\"\"\"\n",
        "  doc = nlp(text)\n",
        "  features = pd.DataFrame(columns=['token', 'pos','head','children'])\n",
        "  for token in doc:\n",
        "    features = features.append({'token': token.text,\n",
        "                                'pos': token.pos_,\n",
        "                                # 'dep': token.dep_,\n",
        "                                'head': token.head.text,\n",
        "                                # 'head_pos': token.head.pos_,\n",
        "                                'children': [child for child in token.children]},\n",
        "                               ignore_index=True)\n",
        "  print(features)\n",
        "  features['dep_wt'] = features.apply(lambda row: len(row['children']) + 1, axis=1)\n",
        "  features['dep_wt_log'] = features.apply(lambda row: math.log(row['dep_wt'], 10) + 1, axis=1)\n",
        "  print(features)\n",
        "  return features\n",
        "  # features = get_ants(features)\n",
        "  # return features\n",
        "\n",
        "def get_ants(token_df):\n",
        "  '''Generate antonyms'''\n",
        "  pos_keys = {'VERB': 'v', 'ADV': 'r', 'ADJ': 'a', 'NOUN': 'n'}\n",
        "  ants = []\n",
        "  for w,p in zip(token_df.token, token_df.pos):\n",
        "    if p in pos_keys.keys():                      # only consider verb, adv, adj, and noun\n",
        "      print(w)\n",
        "      for s in wn.synsets(w, pos=pos_keys[p]):\n",
        "        print(s)\n",
        "        for l in s.lemmas():\n",
        "          print(l)\n",
        "          if l.antonyms():\n",
        "            print(l.antonyms())\n",
        "            ants.append(l.antonyms()[0].name())   # get first antonym for any sense\n",
        "            print('added')\n",
        "            break\n",
        "          else:\n",
        "            ants.append(None)\n",
        "            print('added')\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "        break\n",
        "      else:\n",
        "        ants.append(None)\n",
        "    else:\n",
        "      print(w)\n",
        "      ants.append(None)\n",
        "      print('added')\n",
        "  print(len(ants))\n",
        "  token_df.insert(len(token_df.columns), \"ant\", ants)\n",
        "  return token_df\n",
        "\n",
        "features = get_features('an area of ground where plants such as flowers or vegetables are grown')\n",
        "get_ants(features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lrv2T7cuaT2"
      },
      "source": [
        "Component Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxPkBtHPuaBz"
      },
      "source": [
        "def product(x):\n",
        "  '''Calculate the product of a series'''\n",
        "  prod = 1\n",
        "  for w in x:\n",
        "    prod = prod * w\n",
        "  return prod\n",
        "\n",
        "def get_ants(token_df):\n",
        "  '''Generate antonyms'''\n",
        "  pos_keys = {'VERB': 'v', 'ADV': 'r', 'ADJ': 'a', 'NOUN': 'n'}\n",
        "  ants = []\n",
        "  for w,p in zip(token_df.token, token_df.pos):\n",
        "    if p in pos_keys.keys():                      # only consider verb, adv, adj, and noun\n",
        "      for s in wn.synsets(w, pos=pos_keys[p]):\n",
        "        for l in s.lemmas():\n",
        "          if l.antonyms():\n",
        "            ants.append(l.antonyms()[0].name())   # get first antonym for any sense\n",
        "            break\n",
        "          else:\n",
        "            ants.append(None)\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "        break\n",
        "      else:\n",
        "        ants.append(None)\n",
        "    else:\n",
        "      ants.append(None)\n",
        "  token_df.insert(len(token_df.columns), \"ant\", ants)\n",
        "  return token_df\n",
        "\n",
        "def get_features(text):\n",
        "  \"\"\"Extract linguistic features via Spacy. Also calculate dependency weights\"\"\"\n",
        "  doc = nlp(text)\n",
        "  features = pd.DataFrame(columns=['token', 'pos','head','children'])\n",
        "  for token in doc:\n",
        "    features = features.append({'token': token.text,\n",
        "                                'pos': token.pos_,\n",
        "                                # 'dep': token.dep_,\n",
        "                                'head': token.head.text,\n",
        "                                # 'head_pos': token.head.pos_,\n",
        "                                'children': [child for child in token.children]},\n",
        "                               ignore_index=True)\n",
        "  features['dep_wt'] = features.apply(lambda row: len(row['children']) + 1, axis=1)\n",
        "  features['dep_wt_log'] = features.apply(lambda row: math.log(row['dep_wt'], 10) + 1, axis=1)\n",
        "  features = get_ants(features)\n",
        "  return features\n",
        "\n",
        "innerA = lambda x: [w for w,wt in x]\n",
        "innerB = lambda x: [sum(model[w]*wt for w,wt in x)]\n",
        "innerC = lambda x: [product(model[w]*wt for w,wt in x)]\n",
        "\n",
        "outerA = lambda x,y: model.most_similar(positive=x, negative=y, topn=20)\n",
        "outerB = lambda x,y: model.most_similar_cosmul(positive=x, negative=y, topn=20)\n",
        "\n",
        "combA = (innerA, outerA)  # 3CosAdd\n",
        "combB = (innerA, outerB)  # 3CosMul\n",
        "combC = (innerB, outerA)  # Add\n",
        "combD = (innerC, outerA)  # Mul\n",
        "\n",
        "wtA = lambda x: [1]*len(x)\n",
        "wtB = lambda x: x.dep_wt\n",
        "wtC = lambda x: x.dep_wt_log\n",
        "\n",
        "stopA = lambda x: x   # Return dataframe with stopwords\n",
        "def stopB(tokens_df):\n",
        "  '''Return dataframe without stopwords'''\n",
        "  stopless = remove_stopwords(' '.join(tokens_df['token'])).split()\n",
        "  return tokens_df[tokens_df['token'].isin(stopless)]\n",
        "\n",
        "negA = lambda x: [] # Don't include antonyms\n",
        "negB = lambda x: x  # Include antonyms\n",
        "\n",
        "combs = [combA,combB,combC,combD]\n",
        "wts = [wtA,wtB,wtC]\n",
        "stops = [stopA,stopB] \n",
        "negs = [negA,negB]\n",
        "\n",
        "def compose(tokens_df, comb_type, wt_type, stop_type, neg_type):\n",
        "  stop_df = stop_type(tokens_df)                                          # focus on non-stopwords\n",
        "  p = comb_type[0](zip(stop_df.token, wt_type(stop_df)))                  # combine positives\n",
        "  ants_df = stop_df[stop_df['ant'].values != None]                        # focus on antonyms\n",
        "  if len(ants_df) == 0:\n",
        "    n = []\n",
        "  else:\n",
        "    n = comb_type[0](zip(ants_df.ant, wt_type(ants_df)))                    # combine negatives\n",
        "  most_sim = comb_type[1](p, neg_type(n))\n",
        "  return [w[0] for w in most_sim if w[0] not in list(stop_df.token)][:10]    # first 10 original words to be generated\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6NxT7_Wz2bs"
      },
      "source": [
        "Data collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoL5DqS5_KMU"
      },
      "source": [
        "#Initiate storage dict\n",
        "total = {}\n",
        "scores = {}\n",
        "for i in range(24):\n",
        "  scores[i] = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6vC-ABiz6RM",
        "outputId": "c3e5f87c-0bb1-4e39-e626-3bbdc80c9679"
      },
      "source": [
        "word = 'student'\n",
        "description = \"a person who attends a school, college, or university\"\n",
        "\n",
        "total[word] = [description]\n",
        "\n",
        "new = []\n",
        "\n",
        "raw_string = total[word][0]\n",
        "word_string = re.sub(r'[^\\w\\s]', '', raw_string)\n",
        "\n",
        "tokens = get_features(word_string)\n",
        "print(tokens)\n",
        "print()\n",
        "print(stopB(tokens))\n",
        "print()\n",
        "\n",
        "i,j,k,l,m = 0,0,0,0,0\n",
        "for comb in combs:\n",
        "  i+=1\n",
        "  j,k,l = 0,0,0\n",
        "  if comb == combC:\n",
        "    for wt in wts:\n",
        "      j+=1\n",
        "      k,l = 0,0\n",
        "      for stop in stops:\n",
        "        k+=1\n",
        "        l = 0\n",
        "        for neg in negs:\n",
        "          l+=1\n",
        "          m+=1\n",
        "          result = compose(tokens, comb, wt, stop, neg)\n",
        "          new.append(result)\n",
        "          print(m, '--', 'comb:', i,'wt:', j,'stop:', k,'neg:', l, result)\n",
        "  else:\n",
        "    wt = wtA\n",
        "    for stop in stops:\n",
        "      k+=1\n",
        "      l = 0\n",
        "      for neg in negs:\n",
        "        l+=1\n",
        "        m+=1\n",
        "        result = compose(tokens, comb, wt, stop, neg)\n",
        "        new.append(result)\n",
        "        print(m, '--', 'comb:', i,'wt:', j,'stop:', k,'neg:', l, result)\n",
        "\n",
        "total[word].append(np.array(new))\n",
        "total[word][1]\n",
        "\n",
        "locations = np.where(total[word][1] == word)\n",
        "\n",
        "scores_new = {}\n",
        "for i in range(24):\n",
        "  scores_new[i] = 0\n",
        "\n",
        "for i in range(len(locations[0])):\n",
        "  scores_new[locations[0][i]] = 10 - locations[1][i]\n",
        "\n",
        "for key in scores_new.keys():\n",
        "  scores[key].append(scores_new[key])\n",
        "\n",
        "print(scores)\n",
        "print(scores_new)\n",
        "\n",
        "# compose(tokens, combC, wtA, stopB, negA)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        token    pos     head                     children  dep_wt  \\\n",
            "0           a    DET   person                           []       1   \n",
            "1      person   NOUN   person                 [a, attends]       3   \n",
            "2         who   PRON  attends                           []       1   \n",
            "3     attends   VERB   person               [who, college]       3   \n",
            "4           a    DET  college                           []       1   \n",
            "5      school   NOUN  college                           []       1   \n",
            "6     college   NOUN  attends  [a, school, or, university]       5   \n",
            "7          or  CCONJ  college                           []       1   \n",
            "8  university   NOUN  college                           []       1   \n",
            "\n",
            "   dep_wt_log   ant  \n",
            "0    1.000000  None  \n",
            "1    1.477121  None  \n",
            "2    1.000000  None  \n",
            "3    1.477121  miss  \n",
            "4    1.000000  None  \n",
            "5    1.000000  None  \n",
            "6    1.698970  None  \n",
            "7    1.000000  None  \n",
            "8    1.000000  None  \n",
            "\n",
            "        token   pos     head                     children  dep_wt  dep_wt_log  \\\n",
            "1      person  NOUN   person                 [a, attends]       3    1.477121   \n",
            "3     attends  VERB   person               [who, college]       3    1.477121   \n",
            "5      school  NOUN  college                           []       1    1.000000   \n",
            "6     college  NOUN  attends  [a, school, or, university]       5    1.698970   \n",
            "8  university  NOUN  college                           []       1    1.000000   \n",
            "\n",
            "    ant  \n",
            "1  None  \n",
            "3  miss  \n",
            "5  None  \n",
            "6  None  \n",
            "8  None  \n",
            "\n",
            "1 -- comb: 1 wt: 0 stop: 1 neg: 1 ['another', 'one', 'student', 'he', 'only', 'where', 'students', 'same', ',', 'is']\n",
            "2 -- comb: 1 wt: 0 stop: 1 neg: 2 ['another', 'student', 'one', 'students', 'he', 'only', 'where', 'same', 'is', '.']\n",
            "3 -- comb: 1 wt: 0 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'campus', 'graduating', 'schools', 'attend', 'graduated']\n",
            "4 -- comb: 1 wt: 0 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'yale', 'education', 'graduating']\n",
            "5 -- comb: 2 wt: 0 stop: 1 neg: 1 ['student', 'one', 'another', 'he', 'only', 'where', 'students', ',', 'same', 'is']\n",
            "6 -- comb: 2 wt: 0 stop: 1 neg: 2 ['student', 'another', 'one', 'he', 'students', 'only', 'where', 'is', 'same', '.']\n",
            "7 -- comb: 2 wt: 0 stop: 2 neg: 1 ['student', 'graduate', 'students', 'attended', 'attending', 'campus', 'attend', 'graduating', 'schools', 'teacher']\n",
            "8 -- comb: 2 wt: 0 stop: 2 neg: 2 ['student', 'graduate', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'education', 'yale', 'faculty']\n",
            "9 -- comb: 3 wt: 1 stop: 1 neg: 1 ['student', 'another', 'one', 'students', 'he', 'where', 'only', 'graduate', ',', 'same']\n",
            "10 -- comb: 3 wt: 1 stop: 1 neg: 2 ['student', 'students', 'another', 'one', 'he', 'graduate', 'where', 'only', 'same', '.']\n",
            "11 -- comb: 3 wt: 1 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'campus', 'attending', 'graduating', 'graduated', 'schools', 'yale']\n",
            "12 -- comb: 3 wt: 1 stop: 2 neg: 2 ['graduate', 'student', 'students', 'campus', 'attended', 'undergraduate', 'yale', 'graduating', 'faculty', 'education']\n",
            "13 -- comb: 3 wt: 2 stop: 1 neg: 1 ['student', 'graduate', 'students', 'attended', 'attending', 'he', 'where', 'attend', 'education', 'teacher']\n",
            "14 -- comb: 3 wt: 2 stop: 1 neg: 2 ['student', 'graduate', 'students', 'attended', 'attending', 'education', 'campus', 'colleges', 'undergraduate', 'harvard']\n",
            "15 -- comb: 3 wt: 2 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'graduating', 'campus', 'colleges', 'attend', 'graduated']\n",
            "16 -- comb: 3 wt: 2 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'undergraduate', 'campus', 'colleges', 'graduating', 'attending', 'yale']\n",
            "17 -- comb: 3 wt: 3 stop: 1 neg: 1 ['student', 'students', 'graduate', 'he', 'one', 'where', 'another', 'only', 'attended', ',']\n",
            "18 -- comb: 3 wt: 3 stop: 1 neg: 2 ['student', 'students', 'graduate', 'he', 'where', 'another', 'one', 'education', 'attended', 'only']\n",
            "19 -- comb: 3 wt: 3 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'campus', 'graduating', 'graduated', 'attend', 'schools']\n",
            "20 -- comb: 3 wt: 3 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'graduating', 'yale', 'education']\n",
            "21 -- comb: 4 wt: 0 stop: 1 neg: 1 ['they', 'hundreds', 'instead', 'those', 'addition', 'dozen', 'dozens', 'others', 'outside', 'only']\n",
            "22 -- comb: 4 wt: 0 stop: 1 neg: 2 ['hundreds', 'they', 'instead', 'those', 'addition', 'dozens', 'dozen', 'others', 'outside', 'only']\n",
            "23 -- comb: 4 wt: 0 stop: 2 neg: 1 ['instead', 'they', 'those', 'hundreds', 'addition', 'dozen', 'only', 'others', 'outside', 'dozens']\n",
            "24 -- comb: 4 wt: 0 stop: 2 neg: 2 ['hundreds', 'dozens', 'residents', 'dozen', 'thousands', 'use', 'outside', 'using', 'instead', 'additional']\n",
            "{0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8], 1: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9], 2: [0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 5, 9, 0, 0, 0, 9, 0, 0, 0, 0, 8, 10, 8, 0, 9], 3: [0, 0, 5, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0, 9], 4: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 5: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 6: [0, 1, 6, 0, 0, 0, 7, 0, 0, 0, 4, 10, 0, 0, 0, 9, 0, 0, 0, 0, 8, 10, 5, 0, 10], 7: [0, 1, 5, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 5, 0, 10], 8: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 10: [0, 6, 6, 0, 0, 0, 7, 0, 0, 0, 8, 9, 0, 0, 0, 9, 0, 0, 0, 0, 8, 10, 6, 0, 9], 11: [0, 6, 4, 0, 0, 0, 8, 0, 0, 0, 5, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 6, 0, 9], 12: [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 10], 13: [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 10], 14: [0, 6, 0, 0, 0, 0, 8, 0, 0, 0, 10, 5, 0, 0, 0, 9, 5, 0, 0, 0, 8, 7, 8, 0, 9], 15: [0, 6, 0, 0, 0, 0, 8, 0, 0, 0, 10, 0, 0, 0, 0, 9, 5, 0, 0, 0, 0, 7, 8, 0, 9], 16: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 17: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 18: [0, 7, 0, 0, 0, 0, 7, 0, 0, 0, 9, 8, 0, 0, 0, 9, 3, 0, 0, 0, 8, 10, 8, 0, 9], 19: [0, 7, 0, 0, 0, 0, 8, 0, 0, 0, 9, 0, 0, 0, 0, 9, 3, 0, 0, 0, 0, 7, 8, 0, 9], 20: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 21: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 22: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 23: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "{0: 8, 1: 9, 2: 9, 3: 9, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 9, 11: 9, 12: 10, 13: 10, 14: 9, 15: 9, 16: 10, 17: 10, 18: 9, 19: 9, 20: 0, 21: 0, 22: 0, 23: 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytTITH2UAAnX",
        "outputId": "a3b27cc7-ffa5-46cd-f38e-64e532d625d2"
      },
      "source": [
        "\n",
        "# locations = np.where(total[word][1] == word)\n",
        "\n",
        "# scores_new = {}\n",
        "# for i in range(24):\n",
        "#   scores_new[i] = 0\n",
        "\n",
        "# for i in range(len(locations[0])):\n",
        "#   scores_new[locations[0][i]] = locations[1][i]\n",
        "\n",
        "# for key in scores_new.keys():\n",
        "#   scores[key].append(scores_new[key])\n",
        "\n",
        "# print(scores)\n",
        "# print(scores_new)\n",
        "# del total['stop']\n",
        "\n",
        "print(len(total.keys()))\n",
        "print(total.keys())\n",
        "print([part[0] for part in list(total.values())])\n",
        "scores\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25\n",
            "dict_keys(['attack', 'breakfast', 'chocolate', 'different', 'expensive', 'foreign', 'grandfather', 'healthy', 'important', 'join', 'knife', 'length', 'medicine', 'newspaper', 'obey', 'pain', 'question', 'radio', 'salt', 'tool', 'understand', 'voice', 'why', 'young', 'student'])\n",
            "['to act violently against', 'the first meal of the day', 'a food that is made from cacao beans and that is eaten as candy or used as a flavoring ingredient in other sweet foods', 'partly or totally unlike', 'costing a lot of money', 'coming from or belonging to a different place or country', 'the father of your father or mother', 'having good health', 'deserving or requiring serious attention', 'to put or bring (two or more things) together', 'a usually sharp blade attached to a handle that is used for cutting or as a weapon', 'a measurement of how long something is', 'a substance that is used in treating disease or relieving pain and that is usually in the form of a pill or a liquid', 'a set of large sheets of paper that have news stories, information about local events, advertisements, etc., and that are folded together and sold every day or every week', 'to do what someone tells you to do or what a rule, law, etc., says you must do', 'the physical feeling caused by disease, injury, or something that hurts the body', \"a sentence, phrase, or word that asks for information or is used to test someone's knowledge\", 'the system or process that is used for sending and receiving signals through the air without using wires', 'a natural white substance that is used especially to flavor or preserve food', 'something (such as a hammer, saw, shovel, etc.) that you hold in your hand and use for a particular task', 'to know the meaning of', 'the sounds that you make with your mouth and throat when you are speaking, singing, etc.', 'for what reason or purpose', 'in an early stage of life, growth, or development', 'a person who attends a school, college, or university']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8],\n",
              " 1: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9],\n",
              " 2: [0,\n",
              "  0,\n",
              "  6,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  6,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  5,\n",
              "  9,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  10,\n",
              "  8,\n",
              "  0,\n",
              "  9],\n",
              " 3: [0,\n",
              "  0,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  8,\n",
              "  0,\n",
              "  9],\n",
              " 4: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10],\n",
              " 5: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10],\n",
              " 6: [0,\n",
              "  1,\n",
              "  6,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  7,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  4,\n",
              "  10,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  10,\n",
              "  5,\n",
              "  0,\n",
              "  10],\n",
              " 7: [0,\n",
              "  1,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  5,\n",
              "  0,\n",
              "  10],\n",
              " 8: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10],\n",
              " 9: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10],\n",
              " 10: [0,\n",
              "  6,\n",
              "  6,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  7,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  9,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  10,\n",
              "  6,\n",
              "  0,\n",
              "  9],\n",
              " 11: [0,\n",
              "  6,\n",
              "  4,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  6,\n",
              "  0,\n",
              "  9],\n",
              " 12: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  7,\n",
              "  0,\n",
              "  10],\n",
              " 13: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  3,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  7,\n",
              "  0,\n",
              "  10],\n",
              " 14: [0,\n",
              "  6,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  7,\n",
              "  8,\n",
              "  0,\n",
              "  9],\n",
              " 15: [0,\n",
              "  6,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  5,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  7,\n",
              "  8,\n",
              "  0,\n",
              "  9],\n",
              " 16: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10],\n",
              " 17: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  10],\n",
              " 18: [0,\n",
              "  7,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  7,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  3,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  10,\n",
              "  8,\n",
              "  0,\n",
              "  9],\n",
              " 19: [0,\n",
              "  7,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  8,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  9,\n",
              "  3,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  7,\n",
              "  8,\n",
              "  0,\n",
              "  9],\n",
              " 20: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 21: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 22: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " 23: [0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12EUmfBOg09L",
        "outputId": "7c2abd91-84f2-4708-a387-77c1b879f331"
      },
      "source": [
        "scores_df = pd.DataFrame()\n",
        "for item in scores.keys():\n",
        "  scores_df = scores_df.append(pd.Series(scores[item]), ignore_index=True)\n",
        "scores_df.columns = total.keys()\n",
        "# print(scores_df)\n",
        "\n",
        "# tots = scores_df.sum(axis=1)\n",
        "# print(tots)\n",
        "\n",
        "scores_df.insert(len(scores_df.keys()), 'total', scores_df.sum(axis=1))\n",
        "\n",
        "print(scores_df)\n",
        "\n",
        "\n",
        "# scores_df.to_csv('/content/drive/MyDrive/Colab Notebooks/compsem_scores_50.csv')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    attack  breakfast  chocolate  different  expensive  foreign  grandfather  \\\n",
            "0      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "1      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "2      0.0        0.0        6.0        0.0        0.0      0.0          6.0   \n",
            "3      0.0        0.0        5.0        0.0        0.0      0.0          8.0   \n",
            "4      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "5      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "6      0.0        1.0        6.0        0.0        0.0      0.0          7.0   \n",
            "7      0.0        1.0        5.0        0.0        0.0      0.0          8.0   \n",
            "8      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "9      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "10     0.0        6.0        6.0        0.0        0.0      0.0          7.0   \n",
            "11     0.0        6.0        4.0        0.0        0.0      0.0          8.0   \n",
            "12     0.0        0.0        0.0        0.0        0.0      0.0          5.0   \n",
            "13     0.0        0.0        0.0        0.0        0.0      0.0          3.0   \n",
            "14     0.0        6.0        0.0        0.0        0.0      0.0          8.0   \n",
            "15     0.0        6.0        0.0        0.0        0.0      0.0          8.0   \n",
            "16     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "17     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "18     0.0        7.0        0.0        0.0        0.0      0.0          7.0   \n",
            "19     0.0        7.0        0.0        0.0        0.0      0.0          8.0   \n",
            "20     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "21     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "22     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "23     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n",
            "\n",
            "    healthy  important  join  knife  length  medicine  newspaper  obey  pain  \\\n",
            "0       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "1       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "2       0.0        0.0   0.0    5.0     9.0       0.0        0.0   0.0   9.0   \n",
            "3       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   8.0   \n",
            "4       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "5       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "6       0.0        0.0   0.0    4.0    10.0       0.0        0.0   0.0   9.0   \n",
            "7       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   8.0   \n",
            "8       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "9       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "10      0.0        0.0   0.0    8.0     9.0       0.0        0.0   0.0   9.0   \n",
            "11      0.0        0.0   0.0    5.0     0.0       0.0        0.0   0.0   8.0   \n",
            "12      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "13      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "14      0.0        0.0   0.0   10.0     5.0       0.0        0.0   0.0   9.0   \n",
            "15      0.0        0.0   0.0   10.0     0.0       0.0        0.0   0.0   9.0   \n",
            "16      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "17      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "18      0.0        0.0   0.0    9.0     8.0       0.0        0.0   0.0   9.0   \n",
            "19      0.0        0.0   0.0    9.0     0.0       0.0        0.0   0.0   9.0   \n",
            "20      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "21      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "22      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "23      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n",
            "\n",
            "    question  radio  salt  tool  understand  voice  why  young  student  total  \n",
            "0        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      8.0    8.0  \n",
            "1        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      9.0    9.0  \n",
            "2        0.0    0.0   0.0   0.0         8.0   10.0  8.0    0.0      9.0   70.0  \n",
            "3        0.0    0.0   0.0   0.0         0.0    8.0  8.0    0.0      9.0   46.0  \n",
            "4        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n",
            "5        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n",
            "6        0.0    0.0   0.0   0.0         8.0   10.0  5.0    0.0     10.0   70.0  \n",
            "7        0.0    0.0   0.0   0.0         0.0    8.0  5.0    0.0     10.0   45.0  \n",
            "8        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n",
            "9        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n",
            "10       0.0    0.0   0.0   0.0         8.0   10.0  6.0    0.0      9.0   78.0  \n",
            "11       0.0    0.0   0.0   0.0         0.0    8.0  6.0    0.0      9.0   54.0  \n",
            "12       0.0    0.0   0.0   0.0         0.0    0.0  7.0    0.0     10.0   22.0  \n",
            "13       0.0    0.0   0.0   0.0         0.0    0.0  7.0    0.0     10.0   20.0  \n",
            "14       5.0    0.0   0.0   0.0         8.0    7.0  8.0    0.0      9.0   75.0  \n",
            "15       5.0    0.0   0.0   0.0         0.0    7.0  8.0    0.0      9.0   62.0  \n",
            "16       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n",
            "17       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n",
            "18       3.0    0.0   0.0   0.0         8.0   10.0  8.0    0.0      9.0   78.0  \n",
            "19       3.0    0.0   0.0   0.0         0.0    7.0  8.0    0.0      9.0   60.0  \n",
            "20       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n",
            "21       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n",
            "22       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n",
            "23       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJb345We7bEB"
      },
      "source": [
        "scores_df.to_csv('/content/drive/MyDrive/Colab Notebooks/compsem_scores_50.csv')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mnZoDOV8i6P"
      },
      "source": [
        "Test models/functions (old)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C268rH1PwvE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ddd5b88-5c86-405c-917d-53496f88bfb8"
      },
      "source": [
        "'''word_string = \"a very small animal that has a pointed nose and a long, thin tail\"\n",
        "\n",
        "tokens = get_features(word_string)\n",
        "tokens = add_dep_wts(tokens)\n",
        "tokens = add_dep_wts_log(tokens)\n",
        "tokens = get_ants(tokens)\n",
        "words = tokens['token']\n",
        "\n",
        "\n",
        "# get antonyms - check with mouse def\n",
        "# ants = []\n",
        "# for w in words:\n",
        "#   if wn.synsets(w):\n",
        "#     s = wn.synsets(w)[0]\n",
        "#     if s.lemmas():\n",
        "#       l = s.lemmas()[0]\n",
        "#       if l.antonyms():\n",
        "#         ants.append(l.antonyms()[0].name())\n",
        "# print('antonyms:', ants)\n",
        "\n",
        "basic = lambda x: [w for w in x]\n",
        "ave = lambda x: [sum(model[w] for w in x)/len(x)]\n",
        "added = lambda x: [sum(model[w] for w in x)]\n",
        "\n",
        "def wt_add(tokens_df):\n",
        "  words = tokens_df['token']\n",
        "  redist = []\n",
        "  for w in words: #multiply vectors by dependency weights\n",
        "    redist.append(model[w] * int(list(tokens_df['dep_wt_log'][tokens_df['token'] == w])[0]))\n",
        "  return [sum(redist)]\n",
        "\n",
        "def wt_add_unstop(tokens_df):\n",
        "  words = remove_stopwords(' '.join(tokens_df['token'])).split()\n",
        "  # filtered_df = tokens_df[tokens_df.token.isin(words)]\n",
        "  redist = []\n",
        "  for w in words: #multiply vectors by dependency weights\n",
        "    redist.append(model[w] * int(list(tokens_df['dep_wt_log'][tokens_df['token'] == w])[0]))\n",
        "  return [sum(redist)]\n",
        "\n",
        "def mult(x):\n",
        "  prod = 1\n",
        "  for w in x:\n",
        "    prod = prod * model[w]\n",
        "  return [prod]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df.insert(len(df.columns), \"CosAdd\", [w[0] for w in model.most_similar(positive=basic(words))])\n",
        "df.insert(len(df.columns), \"CosMult\", [w[0] for w in model.most_similar_cosmul(positive=basic(words))])\n",
        "df.insert(len(df.columns), \"Added\", [w[0] for w in model.most_similar(positive=added(words))])\n",
        "df.insert(len(df.columns), \"Weighted\", [w[0] for w in model.most_similar(positive=wt_add(tokens))])\n",
        "df.insert(len(df.columns), \"Weighted_No_Stops\", [w[0] for w in model.most_similar(positive=wt_add_unstop(tokens))])\n",
        "# df.insert(len(df.columns), \"Mult\", [w[0] for w in model.most_similar(positive=mult(words))])\n",
        "\n",
        "df.insert(len(df.columns), \"CosAdd_neg\", [w[0] for w in model.most_similar(positive=basic(words), negative=ants)])\n",
        "df.insert(len(df.columns), \"CosMult_neg\", [w[0] for w in model.most_similar_cosmul(positive=basic(words), negative=ants)])\n",
        "df.insert(len(df.columns), \"Added_neg\", [w[0] for w in model.most_similar(positive=added(words), negative=ants)])\n",
        "df.insert(len(df.columns), \"Weighted_neg\", [w[0] for w in model.most_similar(positive=wt_add(tokens), negative=ants)])\n",
        "df.insert(len(df.columns), \"Weighted_No_Stops_neg\", [w[0] for w in model.most_similar(positive=wt_add_unstop(tokens), negative=ants)])\n",
        "# df.insert(len(df.columns), \"Mult\", [w[0] for w in model.most_similar(positive=mult(words), negative=ants)])\n",
        "\n",
        "\n",
        "# df.insert(len(df.columns), \"Weighted Ave\", [w[0] for w in model.most_similar(positive=[model[words[0]] + model[words[1]] + 2*model[words[2]]])])\n",
        "print()\n",
        "print(df)\n",
        "\n",
        "print()\n",
        "print(tokens)\n",
        "'''"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    CosAdd  CosMult Added Weighted Weighted_No_Stops CosAdd_neg CosMult_neg  \\\n",
            "0      but      but     a        a              tail        but         but   \n",
            "1       so       so   but      but              long         so          so   \n",
            "2     well     even  that     that             small        one         one   \n",
            "3     this     well    so       so           pointed       well        even   \n",
            "4     even   though  very     very              nose       this      though   \n",
            "5       it       it  long     long             large      which        well   \n",
            "6    which    which  this     this                 ,       even       which   \n",
            "7   though     this  even     even               but     though        same   \n",
            "8  because  because    it       it                so    another        this   \n",
            "9  another      one  well     well            though         it     another   \n",
            "\n",
            "  Added_neg Weighted_neg Weighted_No_Stops_neg  \n",
            "0         a            a                  tail  \n",
            "1       but          but                  long  \n",
            "2      that         that                 small  \n",
            "3        so           so               pointed  \n",
            "4      very         very                  nose  \n",
            "5      long         long                     ,  \n",
            "6      this         this                 large  \n",
            "7      even         even                   but  \n",
            "8        it           it                    so  \n",
            "9      well         well                though  \n",
            "\n",
            "      token     dep    head head_pos                 children    pos  dep_wt  \\\n",
            "0         a     det  animal     NOUN                       []    DET       1   \n",
            "1      very  advmod   small      ADJ                       []    ADV       1   \n",
            "2     small    amod  animal     NOUN                   [very]    ADJ       2   \n",
            "3    animal    ROOT  animal     NOUN          [a, small, has]   NOUN       4   \n",
            "4      that   nsubj     has      AUX                       []    DET       1   \n",
            "5       has   relcl  animal     NOUN             [that, nose]    AUX       3   \n",
            "6         a     det    nose     NOUN                       []    DET       1   \n",
            "7   pointed    amod    nose     NOUN                       []    ADJ       1   \n",
            "8      nose    dobj     has      AUX  [a, pointed, and, tail]   NOUN       5   \n",
            "9       and      cc    nose     NOUN                       []  CCONJ       1   \n",
            "10        a     det    tail     NOUN                       []    DET       1   \n",
            "11     long    amod    tail     NOUN                       []    ADJ       1   \n",
            "12        ,   punct    tail     NOUN                       []  PUNCT       1   \n",
            "13     thin    amod    tail     NOUN                       []    ADJ       1   \n",
            "14     tail    conj    nose     NOUN       [a, long, ,, thin]   NOUN       5   \n",
            "\n",
            "    dep_wt_log        ant  \n",
            "0     1.000000       None  \n",
            "1     1.000000       None  \n",
            "2     1.301030      large  \n",
            "3     1.602060       None  \n",
            "4     1.000000       None  \n",
            "5     1.477121       None  \n",
            "6     1.000000       None  \n",
            "7     1.000000  pointless  \n",
            "8     1.698970       None  \n",
            "9     1.000000       None  \n",
            "10    1.000000       None  \n",
            "11    1.000000      short  \n",
            "12    1.000000       None  \n",
            "13    1.000000      thick  \n",
            "14    1.698970       None  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}