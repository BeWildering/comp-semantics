{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"CompositionalSemantics.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/BeWildering/comp-semantics/blob/main/CompositionalSemantics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Gz60OPnswvEl"},"source":["Import libraries"]},{"cell_type":"code","metadata":{"id":"vuUP7XRVwvEm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"893b4d7a-6145-4c36-b288-0d852c3e3267"},"source":["import os\n","import urllib.request\n","import numpy as np\n","import pandas as pd\n","import math\n","import re\n","\n","# Get the interactive Tools for Matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","from sklearn.decomposition import PCA\n","\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","from gensim.parsing.preprocessing import remove_stopwords\n","\n","# !python -m spacy download en_core_web_md\n","import spacy\n","import en_core_web_sm\n","nlp = en_core_web_sm.load()\n","\n","import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","\n","#Show all columns of dataframe\n","pd.options.display.max_columns = None\n","pd.options.display.max_rows = None"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8jHiX33P7sNV"},"source":["Load word embeddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zkw5kIZwvEo","outputId":"819ce335-31ea-4772-c867-530977231535"},"source":["if not os.path.exists('/content/glove.6B.zip'):\n","  urllib.request.urlretrieve('https://nlp.stanford.edu/data/glove.6B.zip', 'glove.6B.zip')\n","if not os.path.exists('/content/glove.6B.100d.txt'):\n","  !unzip /content/glove.6B #unzip compressed file\n","\n","glove_file = datapath('/content/glove.6B.300d.txt')\n","\n","tmp_file = \"/content/glove_tmp\"\n","glove2word2vec(glove_file, tmp_file)\n","print(tmp_file)\n","model = KeyedVectors.load_word2vec_format(tmp_file)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Archive:  /content/glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n","/content/glove_tmp\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rsy3JW5jwvEo"},"source":["Demonstrate Gensim word similarity functions"]},{"cell_type":"code","metadata":{"id":"HTLiNpZswvEp","colab":{"base_uri":"https://localhost:8080/","height":231},"outputId":"ccb4839b-53f6-4c47-a36f-f4a2574a125d"},"source":["print(model.most_similar('kant'))\n","print(model.most_similar(negative='opal'))\n","print(model.similarity(\"happy\", \"unhappy\"))\n","\n","result = model.most_similar(positive=['colorful','sky'], negative=['colorless','uncolored','colourless'])\n","print(\"{}: {:.4f}\".format(*result[0]))\n","\n","def analogy(x1, x2, y1):\n","    result = model.most_similar(positive=[y1, x2], negative=[x1])\n","    return result\n","print(analogy('man', 'king', 'woman'))\n","\n","print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n","\n","def display_pca_scatterplot(model, words=None, sample=0):\n","    if words == None:\n","        if sample > 0:\n","            words = np.random.choice(list(model.index2word), sample)\n","        else:\n","            words = [ word for word in model.vocab ]\n","    word_vectors = np.array([model[w] for w in words])\n","    twodim = PCA().fit_transform(word_vectors)[:,:2]\n","    \n","    plt.figure(figsize=(6,6))\n","    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n","    for word, (x,y) in zip(words, twodim):\n","        plt.text(x+0.05, y+0.05, word)\n","\n","display_pca_scatterplot(model, [\"woman\", \"man\", \"boss\", \"professional\", \"amateur\", \"president\",\n","                                \"happy\", \"unhappy\", \"sad\", \"ecstatic\", \"displeased\", \"joyful\"])"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f06e08b83787>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'kant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'opal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"happy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unhappy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'colorful'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sky'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'colorless'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'uncolored'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'colourless'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"4lrv2T7cuaT2"},"source":["Component Functions"]},{"cell_type":"code","metadata":{"id":"gxPkBtHPuaBz"},"source":["def product(x):\n","  '''Calculate the product of a series'''\n","  prod = 1\n","  for w in x:\n","    prod = prod * w\n","  return prod\n","\n","def get_ants(token_df):\n","  '''Generate antonyms'''\n","  pos_keys = {'VERB': 'v', 'ADV': 'r', 'ADJ': 'a', 'NOUN': 'n'}\n","  ants = []\n","  for w,p in zip(token_df.token, token_df.pos):\n","    if p in pos_keys.keys():                      # only consider verb, adv, adj, and noun\n","      for s in wn.synsets(w, pos=pos_keys[p]):\n","        for l in s.lemmas():\n","          if l.antonyms():\n","            ants.append(l.antonyms()[0].name())   # get first antonym for any sense\n","            break\n","          else:\n","            ants.append(None)\n","          break\n","        else:\n","          continue\n","        break\n","      else:\n","        ants.append(None)\n","    else:\n","      ants.append(None)\n","  token_df.insert(len(token_df.columns), \"ant\", ants)\n","  return token_df\n","\n","def get_features(text):\n","  \"\"\"Extract linguistic features via Spacy. Also calculate dependency weights\"\"\"\n","  doc = nlp(text)\n","  features = pd.DataFrame(columns=['token', 'pos','head','children'])\n","  for token in doc:\n","    features = features.append({'token': token.text,\n","                                'pos': token.pos_,\n","                                # 'dep': token.dep_,\n","                                'head': token.head.text,\n","                                # 'head_pos': token.head.pos_,\n","                                'children': [child for child in token.children]},\n","                               ignore_index=True)\n","  features['dep_wt'] = features.apply(lambda row: len(row['children']) + 1, axis=1)\n","  features['dep_wt_log'] = features.apply(lambda row: math.log(row['dep_wt'], 10) + 1, axis=1)\n","  features = get_ants(features)\n","  return features\n","\n","innerA = lambda x: [w for w,wt in x]\n","innerB = lambda x: [sum(model[w]*wt for w,wt in x)]\n","innerC = lambda x: [product(model[w]*wt for w,wt in x)]\n","\n","outerA = lambda x,y: model.most_similar(positive=x, negative=y, topn=20)\n","outerB = lambda x,y: model.most_similar_cosmul(positive=x, negative=y, topn=20)\n","\n","combA = (innerA, outerA)  # 3CosAdd\n","combB = (innerA, outerB)  # 3CosMul\n","combC = (innerB, outerA)  # Add\n","combD = (innerC, outerA)  # Mul\n","\n","wtA = lambda x: [1]*len(x)\n","wtB = lambda x: x.dep_wt\n","wtC = lambda x: x.dep_wt_log\n","\n","stopA = lambda x: x   # Return dataframe with stopwords\n","def stopB(tokens_df):\n","  '''Return dataframe without stopwords'''\n","  stopless = remove_stopwords(' '.join(tokens_df['token'])).split()\n","  return tokens_df[tokens_df['token'].isin(stopless)]\n","\n","negA = lambda x: [] # Don't include antonyms\n","negB = lambda x: x  # Include antonyms\n","\n","combs = [combA,combB,combC,combD]\n","wts = [wtA,wtB,wtC]\n","stops = [stopA,stopB] \n","negs = [negA,negB]\n","\n","def compose(tokens_df, comb_type, wt_type, stop_type, neg_type):\n","  stop_df = stop_type(tokens_df)                                          # focus on non-stopwords\n","  p = comb_type[0](zip(stop_df.token, wt_type(stop_df)))                  # combine positives\n","  ants_df = stop_df[stop_df['ant'].values != None]                        # focus on antonyms\n","  if len(ants_df) == 0:\n","    n = []\n","  else:\n","    n = comb_type[0](zip(ants_df.ant, wt_type(ants_df)))                    # combine negatives\n","  most_sim = comb_type[1](p, neg_type(n))\n","  return [w[0] for w in most_sim if w[0] not in list(stop_df.token)][:10]    # first 10 original words to be generated\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6NxT7_Wz2bs"},"source":["Data collection"]},{"cell_type":"code","metadata":{"id":"AoL5DqS5_KMU"},"source":["#Initiate storage dict\n","total = {}\n","scores = {}\n","for i in range(24):\n","  scores[i] = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6vC-ABiz6RM","outputId":"c3e5f87c-0bb1-4e39-e626-3bbdc80c9679"},"source":["word = 'student'\n","description = \"a person who attends a school, college, or university\"\n","\n","total[word] = [description]\n","\n","new = []\n","\n","raw_string = total[word][0]\n","word_string = re.sub(r'[^\\w\\s]', '', raw_string)\n","\n","tokens = get_features(word_string)\n","print(tokens)\n","print()\n","print(stopB(tokens))\n","print()\n","\n","i,j,k,l,m = 0,0,0,0,0\n","for comb in combs:\n","  i+=1\n","  j,k,l = 0,0,0\n","  if comb == combC:\n","    for wt in wts:\n","      j+=1\n","      k,l = 0,0\n","      for stop in stops:\n","        k+=1\n","        l = 0\n","        for neg in negs:\n","          l+=1\n","          m+=1\n","          result = compose(tokens, comb, wt, stop, neg)\n","          new.append(result)\n","          print(m, '--', 'comb:', i,'wt:', j,'stop:', k,'neg:', l, result)\n","  else:\n","    wt = wtA\n","    for stop in stops:\n","      k+=1\n","      l = 0\n","      for neg in negs:\n","        l+=1\n","        m+=1\n","        result = compose(tokens, comb, wt, stop, neg)\n","        new.append(result)\n","        print(m, '--', 'comb:', i,'wt:', j,'stop:', k,'neg:', l, result)\n","\n","total[word].append(np.array(new))\n","total[word][1]\n","\n","locations = np.where(total[word][1] == word)\n","\n","scores_new = {}\n","for i in range(24):\n","  scores_new[i] = 0\n","\n","for i in range(len(locations[0])):\n","  scores_new[locations[0][i]] = 10 - locations[1][i]\n","\n","for key in scores_new.keys():\n","  scores[key].append(scores_new[key])\n","\n","print(scores)\n","print(scores_new)\n","\n","# compose(tokens, combC, wtA, stopB, negA)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["        token    pos     head                     children  dep_wt  \\\n","0           a    DET   person                           []       1   \n","1      person   NOUN   person                 [a, attends]       3   \n","2         who   PRON  attends                           []       1   \n","3     attends   VERB   person               [who, college]       3   \n","4           a    DET  college                           []       1   \n","5      school   NOUN  college                           []       1   \n","6     college   NOUN  attends  [a, school, or, university]       5   \n","7          or  CCONJ  college                           []       1   \n","8  university   NOUN  college                           []       1   \n","\n","   dep_wt_log   ant  \n","0    1.000000  None  \n","1    1.477121  None  \n","2    1.000000  None  \n","3    1.477121  miss  \n","4    1.000000  None  \n","5    1.000000  None  \n","6    1.698970  None  \n","7    1.000000  None  \n","8    1.000000  None  \n","\n","        token   pos     head                     children  dep_wt  dep_wt_log  \\\n","1      person  NOUN   person                 [a, attends]       3    1.477121   \n","3     attends  VERB   person               [who, college]       3    1.477121   \n","5      school  NOUN  college                           []       1    1.000000   \n","6     college  NOUN  attends  [a, school, or, university]       5    1.698970   \n","8  university  NOUN  college                           []       1    1.000000   \n","\n","    ant  \n","1  None  \n","3  miss  \n","5  None  \n","6  None  \n","8  None  \n","\n","1 -- comb: 1 wt: 0 stop: 1 neg: 1 ['another', 'one', 'student', 'he', 'only', 'where', 'students', 'same', ',', 'is']\n","2 -- comb: 1 wt: 0 stop: 1 neg: 2 ['another', 'student', 'one', 'students', 'he', 'only', 'where', 'same', 'is', '.']\n","3 -- comb: 1 wt: 0 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'campus', 'graduating', 'schools', 'attend', 'graduated']\n","4 -- comb: 1 wt: 0 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'yale', 'education', 'graduating']\n","5 -- comb: 2 wt: 0 stop: 1 neg: 1 ['student', 'one', 'another', 'he', 'only', 'where', 'students', ',', 'same', 'is']\n","6 -- comb: 2 wt: 0 stop: 1 neg: 2 ['student', 'another', 'one', 'he', 'students', 'only', 'where', 'is', 'same', '.']\n","7 -- comb: 2 wt: 0 stop: 2 neg: 1 ['student', 'graduate', 'students', 'attended', 'attending', 'campus', 'attend', 'graduating', 'schools', 'teacher']\n","8 -- comb: 2 wt: 0 stop: 2 neg: 2 ['student', 'graduate', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'education', 'yale', 'faculty']\n","9 -- comb: 3 wt: 1 stop: 1 neg: 1 ['student', 'another', 'one', 'students', 'he', 'where', 'only', 'graduate', ',', 'same']\n","10 -- comb: 3 wt: 1 stop: 1 neg: 2 ['student', 'students', 'another', 'one', 'he', 'graduate', 'where', 'only', 'same', '.']\n","11 -- comb: 3 wt: 1 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'campus', 'attending', 'graduating', 'graduated', 'schools', 'yale']\n","12 -- comb: 3 wt: 1 stop: 2 neg: 2 ['graduate', 'student', 'students', 'campus', 'attended', 'undergraduate', 'yale', 'graduating', 'faculty', 'education']\n","13 -- comb: 3 wt: 2 stop: 1 neg: 1 ['student', 'graduate', 'students', 'attended', 'attending', 'he', 'where', 'attend', 'education', 'teacher']\n","14 -- comb: 3 wt: 2 stop: 1 neg: 2 ['student', 'graduate', 'students', 'attended', 'attending', 'education', 'campus', 'colleges', 'undergraduate', 'harvard']\n","15 -- comb: 3 wt: 2 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'graduating', 'campus', 'colleges', 'attend', 'graduated']\n","16 -- comb: 3 wt: 2 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'undergraduate', 'campus', 'colleges', 'graduating', 'attending', 'yale']\n","17 -- comb: 3 wt: 3 stop: 1 neg: 1 ['student', 'students', 'graduate', 'he', 'one', 'where', 'another', 'only', 'attended', ',']\n","18 -- comb: 3 wt: 3 stop: 1 neg: 2 ['student', 'students', 'graduate', 'he', 'where', 'another', 'one', 'education', 'attended', 'only']\n","19 -- comb: 3 wt: 3 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'campus', 'graduating', 'graduated', 'attend', 'schools']\n","20 -- comb: 3 wt: 3 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'graduating', 'yale', 'education']\n","21 -- comb: 4 wt: 0 stop: 1 neg: 1 ['they', 'hundreds', 'instead', 'those', 'addition', 'dozen', 'dozens', 'others', 'outside', 'only']\n","22 -- comb: 4 wt: 0 stop: 1 neg: 2 ['hundreds', 'they', 'instead', 'those', 'addition', 'dozens', 'dozen', 'others', 'outside', 'only']\n","23 -- comb: 4 wt: 0 stop: 2 neg: 1 ['instead', 'they', 'those', 'hundreds', 'addition', 'dozen', 'only', 'others', 'outside', 'dozens']\n","24 -- comb: 4 wt: 0 stop: 2 neg: 2 ['hundreds', 'dozens', 'residents', 'dozen', 'thousands', 'use', 'outside', 'using', 'instead', 'additional']\n","{0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8], 1: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9], 2: [0, 0, 6, 0, 0, 0, 6, 0, 0, 0, 5, 9, 0, 0, 0, 9, 0, 0, 0, 0, 8, 10, 8, 0, 9], 3: [0, 0, 5, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 8, 0, 9], 4: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 5: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 6: [0, 1, 6, 0, 0, 0, 7, 0, 0, 0, 4, 10, 0, 0, 0, 9, 0, 0, 0, 0, 8, 10, 5, 0, 10], 7: [0, 1, 5, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 5, 0, 10], 8: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 9: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 10: [0, 6, 6, 0, 0, 0, 7, 0, 0, 0, 8, 9, 0, 0, 0, 9, 0, 0, 0, 0, 8, 10, 6, 0, 9], 11: [0, 6, 4, 0, 0, 0, 8, 0, 0, 0, 5, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 8, 6, 0, 9], 12: [0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 10], 13: [0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 10], 14: [0, 6, 0, 0, 0, 0, 8, 0, 0, 0, 10, 5, 0, 0, 0, 9, 5, 0, 0, 0, 8, 7, 8, 0, 9], 15: [0, 6, 0, 0, 0, 0, 8, 0, 0, 0, 10, 0, 0, 0, 0, 9, 5, 0, 0, 0, 0, 7, 8, 0, 9], 16: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 17: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10], 18: [0, 7, 0, 0, 0, 0, 7, 0, 0, 0, 9, 8, 0, 0, 0, 9, 3, 0, 0, 0, 8, 10, 8, 0, 9], 19: [0, 7, 0, 0, 0, 0, 8, 0, 0, 0, 9, 0, 0, 0, 0, 9, 3, 0, 0, 0, 0, 7, 8, 0, 9], 20: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 21: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 22: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 23: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","{0: 8, 1: 9, 2: 9, 3: 9, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 9, 11: 9, 12: 10, 13: 10, 14: 9, 15: 9, 16: 10, 17: 10, 18: 9, 19: 9, 20: 0, 21: 0, 22: 0, 23: 0}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ytTITH2UAAnX","outputId":"a3b27cc7-ffa5-46cd-f38e-64e532d625d2"},"source":["\n","# locations = np.where(total[word][1] == word)\n","\n","# scores_new = {}\n","# for i in range(24):\n","#   scores_new[i] = 0\n","\n","# for i in range(len(locations[0])):\n","#   scores_new[locations[0][i]] = locations[1][i]\n","\n","# for key in scores_new.keys():\n","#   scores[key].append(scores_new[key])\n","\n","# print(scores)\n","# print(scores_new)\n","# del total['stop']\n","\n","print(len(total.keys()))\n","print(total.keys())\n","print([part[0] for part in list(total.values())])\n","scores\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["25\n","dict_keys(['attack', 'breakfast', 'chocolate', 'different', 'expensive', 'foreign', 'grandfather', 'healthy', 'important', 'join', 'knife', 'length', 'medicine', 'newspaper', 'obey', 'pain', 'question', 'radio', 'salt', 'tool', 'understand', 'voice', 'why', 'young', 'student'])\n","['to act violently against', 'the first meal of the day', 'a food that is made from cacao beans and that is eaten as candy or used as a flavoring ingredient in other sweet foods', 'partly or totally unlike', 'costing a lot of money', 'coming from or belonging to a different place or country', 'the father of your father or mother', 'having good health', 'deserving or requiring serious attention', 'to put or bring (two or more things) together', 'a usually sharp blade attached to a handle that is used for cutting or as a weapon', 'a measurement of how long something is', 'a substance that is used in treating disease or relieving pain and that is usually in the form of a pill or a liquid', 'a set of large sheets of paper that have news stories, information about local events, advertisements, etc., and that are folded together and sold every day or every week', 'to do what someone tells you to do or what a rule, law, etc., says you must do', 'the physical feeling caused by disease, injury, or something that hurts the body', \"a sentence, phrase, or word that asks for information or is used to test someone's knowledge\", 'the system or process that is used for sending and receiving signals through the air without using wires', 'a natural white substance that is used especially to flavor or preserve food', 'something (such as a hammer, saw, shovel, etc.) that you hold in your hand and use for a particular task', 'to know the meaning of', 'the sounds that you make with your mouth and throat when you are speaking, singing, etc.', 'for what reason or purpose', 'in an early stage of life, growth, or development', 'a person who attends a school, college, or university']\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{0: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8],\n"," 1: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  9],\n"," 2: [0,\n","  0,\n","  6,\n","  0,\n","  0,\n","  0,\n","  6,\n","  0,\n","  0,\n","  0,\n","  5,\n","  9,\n","  0,\n","  0,\n","  0,\n","  9,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  10,\n","  8,\n","  0,\n","  9],\n"," 3: [0,\n","  0,\n","  5,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  8,\n","  0,\n","  9],\n"," 4: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  10],\n"," 5: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  10],\n"," 6: [0,\n","  1,\n","  6,\n","  0,\n","  0,\n","  0,\n","  7,\n","  0,\n","  0,\n","  0,\n","  4,\n","  10,\n","  0,\n","  0,\n","  0,\n","  9,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  10,\n","  5,\n","  0,\n","  10],\n"," 7: [0,\n","  1,\n","  5,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  5,\n","  0,\n","  10],\n"," 8: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  10],\n"," 9: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  10],\n"," 10: [0,\n","  6,\n","  6,\n","  0,\n","  0,\n","  0,\n","  7,\n","  0,\n","  0,\n","  0,\n","  8,\n","  9,\n","  0,\n","  0,\n","  0,\n","  9,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  10,\n","  6,\n","  0,\n","  9],\n"," 11: [0,\n","  6,\n","  4,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  5,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  6,\n","  0,\n","  9],\n"," 12: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  5,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  7,\n","  0,\n","  10],\n"," 13: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  3,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  7,\n","  0,\n","  10],\n"," 14: [0,\n","  6,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  10,\n","  5,\n","  0,\n","  0,\n","  0,\n","  9,\n","  5,\n","  0,\n","  0,\n","  0,\n","  8,\n","  7,\n","  8,\n","  0,\n","  9],\n"," 15: [0,\n","  6,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  10,\n","  0,\n","  0,\n","  0,\n","  0,\n","  9,\n","  5,\n","  0,\n","  0,\n","  0,\n","  0,\n","  7,\n","  8,\n","  0,\n","  9],\n"," 16: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  10],\n"," 17: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  10],\n"," 18: [0,\n","  7,\n","  0,\n","  0,\n","  0,\n","  0,\n","  7,\n","  0,\n","  0,\n","  0,\n","  9,\n","  8,\n","  0,\n","  0,\n","  0,\n","  9,\n","  3,\n","  0,\n","  0,\n","  0,\n","  8,\n","  10,\n","  8,\n","  0,\n","  9],\n"," 19: [0,\n","  7,\n","  0,\n","  0,\n","  0,\n","  0,\n","  8,\n","  0,\n","  0,\n","  0,\n","  9,\n","  0,\n","  0,\n","  0,\n","  0,\n","  9,\n","  3,\n","  0,\n","  0,\n","  0,\n","  0,\n","  7,\n","  8,\n","  0,\n","  9],\n"," 20: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 21: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 22: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 23: [0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0]}"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12EUmfBOg09L","outputId":"7c2abd91-84f2-4708-a387-77c1b879f331"},"source":["scores_df = pd.DataFrame()\n","for item in scores.keys():\n","  scores_df = scores_df.append(pd.Series(scores[item]), ignore_index=True)\n","scores_df.columns = total.keys()\n","# print(scores_df)\n","\n","# tots = scores_df.sum(axis=1)\n","# print(tots)\n","\n","scores_df.insert(len(scores_df.keys()), 'total', scores_df.sum(axis=1))\n","\n","print(scores_df)\n","\n","\n","# scores_df.to_csv('/content/drive/MyDrive/Colab Notebooks/compsem_scores_50.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["    attack  breakfast  chocolate  different  expensive  foreign  grandfather  \\\n","0      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","1      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","2      0.0        0.0        6.0        0.0        0.0      0.0          6.0   \n","3      0.0        0.0        5.0        0.0        0.0      0.0          8.0   \n","4      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","5      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","6      0.0        1.0        6.0        0.0        0.0      0.0          7.0   \n","7      0.0        1.0        5.0        0.0        0.0      0.0          8.0   \n","8      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","9      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","10     0.0        6.0        6.0        0.0        0.0      0.0          7.0   \n","11     0.0        6.0        4.0        0.0        0.0      0.0          8.0   \n","12     0.0        0.0        0.0        0.0        0.0      0.0          5.0   \n","13     0.0        0.0        0.0        0.0        0.0      0.0          3.0   \n","14     0.0        6.0        0.0        0.0        0.0      0.0          8.0   \n","15     0.0        6.0        0.0        0.0        0.0      0.0          8.0   \n","16     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","17     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","18     0.0        7.0        0.0        0.0        0.0      0.0          7.0   \n","19     0.0        7.0        0.0        0.0        0.0      0.0          8.0   \n","20     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","21     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","22     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","23     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","\n","    healthy  important  join  knife  length  medicine  newspaper  obey  pain  \\\n","0       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","1       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","2       0.0        0.0   0.0    5.0     9.0       0.0        0.0   0.0   9.0   \n","3       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   8.0   \n","4       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","5       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","6       0.0        0.0   0.0    4.0    10.0       0.0        0.0   0.0   9.0   \n","7       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   8.0   \n","8       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","9       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","10      0.0        0.0   0.0    8.0     9.0       0.0        0.0   0.0   9.0   \n","11      0.0        0.0   0.0    5.0     0.0       0.0        0.0   0.0   8.0   \n","12      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","13      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","14      0.0        0.0   0.0   10.0     5.0       0.0        0.0   0.0   9.0   \n","15      0.0        0.0   0.0   10.0     0.0       0.0        0.0   0.0   9.0   \n","16      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","17      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","18      0.0        0.0   0.0    9.0     8.0       0.0        0.0   0.0   9.0   \n","19      0.0        0.0   0.0    9.0     0.0       0.0        0.0   0.0   9.0   \n","20      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","21      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","22      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","23      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","\n","    question  radio  salt  tool  understand  voice  why  young  student  total  \n","0        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      8.0    8.0  \n","1        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      9.0    9.0  \n","2        0.0    0.0   0.0   0.0         8.0   10.0  8.0    0.0      9.0   70.0  \n","3        0.0    0.0   0.0   0.0         0.0    8.0  8.0    0.0      9.0   46.0  \n","4        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","5        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","6        0.0    0.0   0.0   0.0         8.0   10.0  5.0    0.0     10.0   70.0  \n","7        0.0    0.0   0.0   0.0         0.0    8.0  5.0    0.0     10.0   45.0  \n","8        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","9        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","10       0.0    0.0   0.0   0.0         8.0   10.0  6.0    0.0      9.0   78.0  \n","11       0.0    0.0   0.0   0.0         0.0    8.0  6.0    0.0      9.0   54.0  \n","12       0.0    0.0   0.0   0.0         0.0    0.0  7.0    0.0     10.0   22.0  \n","13       0.0    0.0   0.0   0.0         0.0    0.0  7.0    0.0     10.0   20.0  \n","14       5.0    0.0   0.0   0.0         8.0    7.0  8.0    0.0      9.0   75.0  \n","15       5.0    0.0   0.0   0.0         0.0    7.0  8.0    0.0      9.0   62.0  \n","16       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","17       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","18       3.0    0.0   0.0   0.0         8.0   10.0  8.0    0.0      9.0   78.0  \n","19       3.0    0.0   0.0   0.0         0.0    7.0  8.0    0.0      9.0   60.0  \n","20       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n","21       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n","22       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n","23       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MJb345We7bEB"},"source":["scores_df.to_csv('/content/drive/MyDrive/Colab Notebooks/compsem_scores_50.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mnZoDOV8i6P"},"source":["Test models/functions (old)"]},{"cell_type":"code","metadata":{"id":"C268rH1PwvE0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6ddd5b88-5c86-405c-917d-53496f88bfb8"},"source":["'''word_string = \"a very small animal that has a pointed nose and a long, thin tail\"\n","\n","tokens = get_features(word_string)\n","tokens = add_dep_wts(tokens)\n","tokens = add_dep_wts_log(tokens)\n","tokens = get_ants(tokens)\n","words = tokens['token']\n","\n","\n","# get antonyms - check with mouse def\n","# ants = []\n","# for w in words:\n","#   if wn.synsets(w):\n","#     s = wn.synsets(w)[0]\n","#     if s.lemmas():\n","#       l = s.lemmas()[0]\n","#       if l.antonyms():\n","#         ants.append(l.antonyms()[0].name())\n","# print('antonyms:', ants)\n","\n","basic = lambda x: [w for w in x]\n","ave = lambda x: [sum(model[w] for w in x)/len(x)]\n","added = lambda x: [sum(model[w] for w in x)]\n","\n","def wt_add(tokens_df):\n","  words = tokens_df['token']\n","  redist = []\n","  for w in words: #multiply vectors by dependency weights\n","    redist.append(model[w] * int(list(tokens_df['dep_wt_log'][tokens_df['token'] == w])[0]))\n","  return [sum(redist)]\n","\n","def wt_add_unstop(tokens_df):\n","  words = remove_stopwords(' '.join(tokens_df['token'])).split()\n","  # filtered_df = tokens_df[tokens_df.token.isin(words)]\n","  redist = []\n","  for w in words: #multiply vectors by dependency weights\n","    redist.append(model[w] * int(list(tokens_df['dep_wt_log'][tokens_df['token'] == w])[0]))\n","  return [sum(redist)]\n","\n","def mult(x):\n","  prod = 1\n","  for w in x:\n","    prod = prod * model[w]\n","  return [prod]\n","\n","df = pd.DataFrame()\n","df.insert(len(df.columns), \"CosAdd\", [w[0] for w in model.most_similar(positive=basic(words))])\n","df.insert(len(df.columns), \"CosMult\", [w[0] for w in model.most_similar_cosmul(positive=basic(words))])\n","df.insert(len(df.columns), \"Added\", [w[0] for w in model.most_similar(positive=added(words))])\n","df.insert(len(df.columns), \"Weighted\", [w[0] for w in model.most_similar(positive=wt_add(tokens))])\n","df.insert(len(df.columns), \"Weighted_No_Stops\", [w[0] for w in model.most_similar(positive=wt_add_unstop(tokens))])\n","# df.insert(len(df.columns), \"Mult\", [w[0] for w in model.most_similar(positive=mult(words))])\n","\n","df.insert(len(df.columns), \"CosAdd_neg\", [w[0] for w in model.most_similar(positive=basic(words), negative=ants)])\n","df.insert(len(df.columns), \"CosMult_neg\", [w[0] for w in model.most_similar_cosmul(positive=basic(words), negative=ants)])\n","df.insert(len(df.columns), \"Added_neg\", [w[0] for w in model.most_similar(positive=added(words), negative=ants)])\n","df.insert(len(df.columns), \"Weighted_neg\", [w[0] for w in model.most_similar(positive=wt_add(tokens), negative=ants)])\n","df.insert(len(df.columns), \"Weighted_No_Stops_neg\", [w[0] for w in model.most_similar(positive=wt_add_unstop(tokens), negative=ants)])\n","# df.insert(len(df.columns), \"Mult\", [w[0] for w in model.most_similar(positive=mult(words), negative=ants)])\n","\n","\n","# df.insert(len(df.columns), \"Weighted Ave\", [w[0] for w in model.most_similar(positive=[model[words[0]] + model[words[1]] + 2*model[words[2]]])])\n","print()\n","print(df)\n","\n","print()\n","print(tokens)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","    CosAdd  CosMult Added Weighted Weighted_No_Stops CosAdd_neg CosMult_neg  \\\n","0      but      but     a        a              tail        but         but   \n","1       so       so   but      but              long         so          so   \n","2     well     even  that     that             small        one         one   \n","3     this     well    so       so           pointed       well        even   \n","4     even   though  very     very              nose       this      though   \n","5       it       it  long     long             large      which        well   \n","6    which    which  this     this                 ,       even       which   \n","7   though     this  even     even               but     though        same   \n","8  because  because    it       it                so    another        this   \n","9  another      one  well     well            though         it     another   \n","\n","  Added_neg Weighted_neg Weighted_No_Stops_neg  \n","0         a            a                  tail  \n","1       but          but                  long  \n","2      that         that                 small  \n","3        so           so               pointed  \n","4      very         very                  nose  \n","5      long         long                     ,  \n","6      this         this                 large  \n","7      even         even                   but  \n","8        it           it                    so  \n","9      well         well                though  \n","\n","      token     dep    head head_pos                 children    pos  dep_wt  \\\n","0         a     det  animal     NOUN                       []    DET       1   \n","1      very  advmod   small      ADJ                       []    ADV       1   \n","2     small    amod  animal     NOUN                   [very]    ADJ       2   \n","3    animal    ROOT  animal     NOUN          [a, small, has]   NOUN       4   \n","4      that   nsubj     has      AUX                       []    DET       1   \n","5       has   relcl  animal     NOUN             [that, nose]    AUX       3   \n","6         a     det    nose     NOUN                       []    DET       1   \n","7   pointed    amod    nose     NOUN                       []    ADJ       1   \n","8      nose    dobj     has      AUX  [a, pointed, and, tail]   NOUN       5   \n","9       and      cc    nose     NOUN                       []  CCONJ       1   \n","10        a     det    tail     NOUN                       []    DET       1   \n","11     long    amod    tail     NOUN                       []    ADJ       1   \n","12        ,   punct    tail     NOUN                       []  PUNCT       1   \n","13     thin    amod    tail     NOUN                       []    ADJ       1   \n","14     tail    conj    nose     NOUN       [a, long, ,, thin]   NOUN       5   \n","\n","    dep_wt_log        ant  \n","0     1.000000       None  \n","1     1.000000       None  \n","2     1.301030      large  \n","3     1.602060       None  \n","4     1.000000       None  \n","5     1.477121       None  \n","6     1.000000       None  \n","7     1.000000  pointless  \n","8     1.698970       None  \n","9     1.000000       None  \n","10    1.000000       None  \n","11    1.000000      short  \n","12    1.000000       None  \n","13    1.000000      thick  \n","14    1.698970       None  \n"],"name":"stdout"}]}]}