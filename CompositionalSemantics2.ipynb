{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"CompositionalSemantics2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/BeWildering/comp-semantics/blob/main/CompositionalSemantics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Gz60OPnswvEl"},"source":["Import libraries, load language models"]},{"cell_type":"code","metadata":{"id":"vuUP7XRVwvEm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621777394311,"user_tz":240,"elapsed":2042,"user":{"displayName":"Brandon Wilde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVVczSsXsm7x6mp0Lr5GzIAsj6EDDkTgWlg_lg=s64","userId":"18127879461253566195"}},"outputId":"24ab0c89-6bfe-4ef2-8a6c-39fe3aa87596"},"source":["import os\n","import urllib.request\n","import numpy as np\n","import pandas as pd\n","import math\n","import re\n","from gensim.test.utils import datapath, get_tmpfile\n","from gensim.models import KeyedVectors\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","from gensim.parsing.preprocessing import remove_stopwords\n","from sklearn.decomposition import PCA\n","import spacy\n","import en_core_web_sm\n","import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet as wn\n","\n","# Get interactive Tools for Matplotlib\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","# Load language model for parsing sentences\n","nlp = en_core_web_sm.load()\n","\n","#Show all columns of dataframe\n","pd.options.display.max_columns = None\n","pd.options.display.max_rows = None"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8jHiX33P7sNV"},"source":["Load word embeddings"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zkw5kIZwvEo","executionInfo":{"status":"ok","timestamp":1621777869569,"user_tz":240,"elapsed":310649,"user":{"displayName":"Brandon Wilde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVVczSsXsm7x6mp0Lr5GzIAsj6EDDkTgWlg_lg=s64","userId":"18127879461253566195"}},"outputId":"91da2347-94db-43cd-b28b-3d3a2b31bda0"},"source":["if not os.path.exists('/content/glove.6B.zip'):\n","  urllib.request.urlretrieve('https://nlp.stanford.edu/data/glove.6B.zip', 'glove.6B.zip')\n","if not os.path.exists('/content/glove.6B.100d.txt'):\n","  !unzip /content/glove.6B\n","\n","glove_file = datapath('/content/glove.6B.300d.txt')\n","\n","tmp_file = \"/content/glove_tmp\"\n","glove2word2vec(glove_file, tmp_file)\n","model = KeyedVectors.load_word2vec_format(tmp_file)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Archive:  /content/glove.6B.zip\n","  inflating: glove.6B.50d.txt        \n","  inflating: glove.6B.100d.txt       \n","  inflating: glove.6B.200d.txt       \n","  inflating: glove.6B.300d.txt       \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4lrv2T7cuaT2"},"source":["Component Functions"]},{"cell_type":"code","metadata":{"id":"gxPkBtHPuaBz","executionInfo":{"status":"ok","timestamp":1621778284070,"user_tz":240,"elapsed":294,"user":{"displayName":"Brandon Wilde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVVczSsXsm7x6mp0Lr5GzIAsj6EDDkTgWlg_lg=s64","userId":"18127879461253566195"}}},"source":["def product(x):\n","  '''Calculate the product of a series'''\n","  prod = 1\n","  for w in x:\n","    prod = prod * w\n","  return prod\n","\n","def get_features(text):\n","  \"\"\"Extracts linguistic features and dependency weights. Returns as dataframe.\"\"\"\n","  doc = nlp(text)\n","  features = pd.DataFrame(columns=['token', 'pos','head','children'])\n","  for token in doc:\n","    features = features.append({'token': token.text,\n","                                'pos': token.pos_,\n","                                # 'dep': token.dep_,\n","                                'head': token.head.text,\n","                                # 'head_pos': token.head.pos_,\n","                                'children': [child for child in token.children]},\n","                               ignore_index=True)\n","  features['dep_wt'] = features.apply(lambda row: len(row['children']) + 1, axis=1)\n","  features['dep_wt_log'] = features.apply(lambda row: math.log(row['dep_wt'], 10) + 1, axis=1)\n","  features = get_ants(features)\n","  return features\n","\n","def get_ants(token_df):\n","  '''Generate antonyms and insert as new column in token features dataframe.'''\n","  pos_keys = {'VERB': 'v', 'ADV': 'r', 'ADJ': 'a', 'NOUN': 'n'}\n","  ants = []\n","  for w,p in zip(token_df.token, token_df.pos):\n","    if p in pos_keys.keys():                      # only consider verb, adv, adj, and noun\n","      for s in wn.synsets(w, pos=pos_keys[p]):\n","        for l in s.lemmas():\n","          if l.antonyms():\n","            ants.append(l.antonyms()[0].name())   # get first antonym for any sense\n","            break\n","          else:\n","            ants.append(None)\n","          break\n","        else:\n","          continue\n","        break\n","      else:\n","        ants.append(None)\n","    else:\n","      ants.append(None)\n","  token_df.insert(len(token_df.columns), \"ant\", ants)\n","  return token_df\n","\n","# Inner combination methods\n","innerA = lambda x: [w for w,wt in x]\n","innerB = lambda x: [sum(model[w]*wt for w,wt in x)]\n","innerC = lambda x: [product(model[w]*wt for w,wt in x)]\n","\n","# Outer combination methods\n","outerA = lambda x,y: model.most_similar(positive=x, negative=y, topn=20)\n","outerB = lambda x,y: model.most_similar_cosmul(positive=x, negative=y, topn=20)\n","\n","# Combination methods\n","combA = (innerA, outerA)  # 3CosAdd\n","combB = (innerA, outerB)  # 3CosMul\n","combC = (innerB, outerA)  # Add\n","combD = (innerC, outerA)  # Mul\n","\n","# Weighting schemes\n","wtA = lambda x: [1]*len(x)\n","wtB = lambda x: x.dep_wt\n","wtC = lambda x: x.dep_wt_log\n","\n","# Stopword removal options\n","stopA = lambda x: x   # Return dataframe with stopwords\n","def stopB(tokens_df):\n","  '''Return dataframe without stopwords'''\n","  stopless = remove_stopwords(' '.join(tokens_df['token'])).split()\n","  return tokens_df[tokens_df['token'].isin(stopless)]\n","\n","# Antonym inclusion options\n","negA = lambda x: [] # Don't include antonyms\n","negB = lambda x: x  # Include antonyms\n","\n","# Parameter sets for iterative testing\n","combs = [combA,combB,combC,combD]\n","wts = [wtA,wtB,wtC]\n","stops = [stopA,stopB] \n","negs = [negA,negB]\n","\n","def compose(tokens_df, comb_type, wt_type, stop_type, neg_type):\n","  '''Generate list of words most similar to the combined meaning of\n","  the input tokens. Takes token features dataframe and parameters as inputs,\n","  returns wordlist.'''\n","  stop_df = stop_type(tokens_df)                                            # handle stopwords\n","  p = comb_type[0](zip(stop_df.token, wt_type(stop_df)))                    # combine positives\n","  ants_df = stop_df[stop_df['ant'].values != None]                          # handle antonyms\n","  if len(ants_df) == 0:\n","    n = []\n","  else:\n","    n = comb_type[0](zip(ants_df.ant, wt_type(ants_df)))                    # combine negatives\n","  most_sim = comb_type[1](p, neg_type(n))                                   # combine positives and negatives\n","  return [w[0] for w in most_sim if w[0] not in list(stop_df.token)][:10]   # first 10 original words to be generated\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6NxT7_Wz2bs"},"source":["Data storage dicts"]},{"cell_type":"code","metadata":{"id":"AoL5DqS5_KMU","executionInfo":{"status":"ok","timestamp":1621779553727,"user_tz":240,"elapsed":149,"user":{"displayName":"Brandon Wilde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVVczSsXsm7x6mp0Lr5GzIAsj6EDDkTgWlg_lg=s64","userId":"18127879461253566195"}}},"source":["#Initiate storage dicts\n","total = {}              # contains words and their full results\n","scores = {}             # contains a running list of scores for each model\n","for i in range(24):\n","  scores[i] = []        # Initiate empty list for each of the 24 models"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2fv2uyCZsy4h"},"source":["Data collection\n","*   Recommended wordlist: https://simple.wikipedia.org/wiki/Wikipedia:List_of_1000_basic_words\n","*   Recommended dictionary: https://www.learnersdictionary.com/\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v6vC-ABiz6RM","executionInfo":{"status":"ok","timestamp":1621781174394,"user_tz":240,"elapsed":2953,"user":{"displayName":"Brandon Wilde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVVczSsXsm7x6mp0Lr5GzIAsj6EDDkTgWlg_lg=s64","userId":"18127879461253566195"}},"outputId":"cd1f4023-60f2-48d3-a04e-feea5a3f290b"},"source":["# Specify word and definition/description to test\n","word = 'student'\n","definition = \"a person who attends a school, college, or university\"\n","total[word] = [definition]\n","\n","new = []\n","raw_string = total[word][0]\n","word_string = re.sub(r'[^\\w\\s]', '', raw_string) #remove any punctuation\n","\n","tokens = get_features(word_string)\n","print('All tokens and features:')\n","print(tokens)\n","print()\n","print('Tokens and features minus stopwords:')\n","print(stopB(tokens))\n","print()\n","\n","# Test all parameter combinations (but wtB and wtC only used with combC)\n","i,m = 0,0\n","for comb in combs:\n","  i+=1\n","  j,k,l = 0,0,0\n","  if comb == combC:\n","    for wt in wts:\n","      j+=1\n","      k,l = 0,0\n","      for stop in stops:\n","        k+=1\n","        l = 0\n","        for neg in negs:\n","          l+=1\n","          m+=1\n","          result = compose(tokens, comb, wt, stop, neg)\n","          new.append(result)\n","          print(m, '--', 'comb:', i,'wt:', j,'stop:', k,'neg:', l, result)\n","  else:\n","    wt = wtA\n","    j = 1\n","    for stop in stops:\n","      k+=1\n","      l = 0\n","      for neg in negs:\n","        l+=1\n","        m+=1\n","        result = compose(tokens, comb, wt, stop, neg)\n","        new.append(result)\n","        print(m, '--', 'comb:', i,'wt:', j,'stop:', k,'neg:', l, result)\n","\n","total[word].append(np.array(new))             # add resulting wordlist to 'total' dict\n","\n","locations = np.where(total[word][1] == word)  # note index of the correct word in top 10\n","\n","# Set default score for each model to 0\n","scores_new = {}\n","for i in range(24):\n","  scores_new[i] = 0\n","\n","# If correct word generated, then add update model score to (10 - index of correct word)\n","for i in range(len(locations[0])):\n","  scores_new[locations[0][i]] = 10 - locations[1][i]\n","\n","# Add scores to storage dict 'scores'\n","for key in scores_new.keys():\n","  scores[key].append(scores_new[key])\n","\n","print()\n","print('Model number: Score')\n","print(scores_new)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["All tokens and features:\n","        token    pos     head                     children  dep_wt  \\\n","0           a    DET   person                           []       1   \n","1      person   NOUN   person                 [a, attends]       3   \n","2         who   PRON  attends                           []       1   \n","3     attends   VERB   person               [who, college]       3   \n","4           a    DET  college                           []       1   \n","5      school   NOUN  college                           []       1   \n","6     college   NOUN  attends  [a, school, or, university]       5   \n","7          or  CCONJ  college                           []       1   \n","8  university   NOUN  college                           []       1   \n","\n","   dep_wt_log   ant  \n","0    1.000000  None  \n","1    1.477121  None  \n","2    1.000000  None  \n","3    1.477121  miss  \n","4    1.000000  None  \n","5    1.000000  None  \n","6    1.698970  None  \n","7    1.000000  None  \n","8    1.000000  None  \n","\n","Tokens and features minus stopwords:\n","        token   pos     head                     children  dep_wt  dep_wt_log  \\\n","1      person  NOUN   person                 [a, attends]       3    1.477121   \n","3     attends  VERB   person               [who, college]       3    1.477121   \n","5      school  NOUN  college                           []       1    1.000000   \n","6     college  NOUN  attends  [a, school, or, university]       5    1.698970   \n","8  university  NOUN  college                           []       1    1.000000   \n","\n","    ant  \n","1  None  \n","3  miss  \n","5  None  \n","6  None  \n","8  None  \n","\n","1 -- comb: 1 wt: 1 stop: 1 neg: 1 ['another', 'one', 'student', 'he', 'only', 'where', 'students', 'same', ',', 'is']\n","2 -- comb: 1 wt: 1 stop: 1 neg: 2 ['another', 'student', 'one', 'students', 'he', 'only', 'where', 'same', 'is', '.']\n","3 -- comb: 1 wt: 1 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'campus', 'graduating', 'schools', 'attend', 'graduated']\n","4 -- comb: 1 wt: 1 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'yale', 'education', 'graduating']\n","5 -- comb: 2 wt: 1 stop: 1 neg: 1 ['student', 'one', 'another', 'he', 'only', 'where', 'students', ',', 'same', 'is']\n","6 -- comb: 2 wt: 1 stop: 1 neg: 2 ['student', 'another', 'one', 'he', 'students', 'only', 'where', 'is', 'same', '.']\n","7 -- comb: 2 wt: 1 stop: 2 neg: 1 ['student', 'graduate', 'students', 'attended', 'attending', 'campus', 'attend', 'graduating', 'schools', 'teacher']\n","8 -- comb: 2 wt: 1 stop: 2 neg: 2 ['student', 'graduate', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'education', 'yale', 'faculty']\n","9 -- comb: 3 wt: 1 stop: 1 neg: 1 ['student', 'another', 'one', 'students', 'he', 'where', 'only', 'graduate', ',', 'same']\n","10 -- comb: 3 wt: 1 stop: 1 neg: 2 ['student', 'students', 'another', 'one', 'he', 'graduate', 'where', 'only', 'same', '.']\n","11 -- comb: 3 wt: 1 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'campus', 'attending', 'graduating', 'graduated', 'schools', 'yale']\n","12 -- comb: 3 wt: 1 stop: 2 neg: 2 ['graduate', 'student', 'students', 'campus', 'attended', 'undergraduate', 'yale', 'graduating', 'faculty', 'education']\n","13 -- comb: 3 wt: 2 stop: 1 neg: 1 ['student', 'graduate', 'students', 'attended', 'attending', 'he', 'where', 'attend', 'education', 'teacher']\n","14 -- comb: 3 wt: 2 stop: 1 neg: 2 ['student', 'graduate', 'students', 'attended', 'attending', 'education', 'campus', 'colleges', 'undergraduate', 'harvard']\n","15 -- comb: 3 wt: 2 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'graduating', 'campus', 'colleges', 'attend', 'graduated']\n","16 -- comb: 3 wt: 2 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'undergraduate', 'campus', 'colleges', 'graduating', 'attending', 'yale']\n","17 -- comb: 3 wt: 3 stop: 1 neg: 1 ['student', 'students', 'graduate', 'he', 'one', 'where', 'another', 'only', 'attended', ',']\n","18 -- comb: 3 wt: 3 stop: 1 neg: 2 ['student', 'students', 'graduate', 'he', 'where', 'another', 'one', 'education', 'attended', 'only']\n","19 -- comb: 3 wt: 3 stop: 2 neg: 1 ['graduate', 'student', 'students', 'attended', 'attending', 'campus', 'graduating', 'graduated', 'attend', 'schools']\n","20 -- comb: 3 wt: 3 stop: 2 neg: 2 ['graduate', 'student', 'students', 'attended', 'campus', 'undergraduate', 'attending', 'graduating', 'yale', 'education']\n","21 -- comb: 4 wt: 1 stop: 1 neg: 1 ['they', 'hundreds', 'instead', 'those', 'addition', 'dozen', 'dozens', 'others', 'outside', 'only']\n","22 -- comb: 4 wt: 1 stop: 1 neg: 2 ['hundreds', 'they', 'instead', 'those', 'addition', 'dozens', 'dozen', 'others', 'outside', 'only']\n","23 -- comb: 4 wt: 1 stop: 2 neg: 1 ['instead', 'they', 'those', 'hundreds', 'addition', 'dozen', 'only', 'others', 'outside', 'dozens']\n","24 -- comb: 4 wt: 1 stop: 2 neg: 2 ['hundreds', 'dozens', 'residents', 'dozen', 'thousands', 'use', 'outside', 'using', 'instead', 'additional']\n","\n","Model number: Score\n","{0: 8, 1: 9, 2: 9, 3: 9, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 9, 11: 9, 12: 10, 13: 10, 14: 9, 15: 9, 16: 10, 17: 10, 18: 9, 19: 9, 20: 0, 21: 0, 22: 0, 23: 0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d8krduQguP-S"},"source":["Display aggregated results (per model)"]},{"cell_type":"code","metadata":{"id":"ytTITH2UAAnX"},"source":["print(len(total.keys()), 'test(s) run')\n","print('Words:', list(total.keys()))\n","print('Definitions:', [part[0] for part in list(total.values())])\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v0tTkvjBvTWC"},"source":["Transfer scores into DataFrame"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12EUmfBOg09L","outputId":"7c2abd91-84f2-4708-a387-77c1b879f331"},"source":["scores_df = pd.DataFrame()\n","for item in scores.keys():\n","  scores_df = scores_df.append(pd.Series(scores[item]), ignore_index=True)\n","scores_df.columns = total.keys()\n","scores_df.insert(len(scores_df.keys()), 'total', scores_df.sum(axis=1))     # add cumulative score for each model\n","\n","print(scores_df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["    attack  breakfast  chocolate  different  expensive  foreign  grandfather  \\\n","0      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","1      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","2      0.0        0.0        6.0        0.0        0.0      0.0          6.0   \n","3      0.0        0.0        5.0        0.0        0.0      0.0          8.0   \n","4      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","5      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","6      0.0        1.0        6.0        0.0        0.0      0.0          7.0   \n","7      0.0        1.0        5.0        0.0        0.0      0.0          8.0   \n","8      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","9      0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","10     0.0        6.0        6.0        0.0        0.0      0.0          7.0   \n","11     0.0        6.0        4.0        0.0        0.0      0.0          8.0   \n","12     0.0        0.0        0.0        0.0        0.0      0.0          5.0   \n","13     0.0        0.0        0.0        0.0        0.0      0.0          3.0   \n","14     0.0        6.0        0.0        0.0        0.0      0.0          8.0   \n","15     0.0        6.0        0.0        0.0        0.0      0.0          8.0   \n","16     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","17     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","18     0.0        7.0        0.0        0.0        0.0      0.0          7.0   \n","19     0.0        7.0        0.0        0.0        0.0      0.0          8.0   \n","20     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","21     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","22     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","23     0.0        0.0        0.0        0.0        0.0      0.0          0.0   \n","\n","    healthy  important  join  knife  length  medicine  newspaper  obey  pain  \\\n","0       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","1       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","2       0.0        0.0   0.0    5.0     9.0       0.0        0.0   0.0   9.0   \n","3       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   8.0   \n","4       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","5       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","6       0.0        0.0   0.0    4.0    10.0       0.0        0.0   0.0   9.0   \n","7       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   8.0   \n","8       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","9       0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","10      0.0        0.0   0.0    8.0     9.0       0.0        0.0   0.0   9.0   \n","11      0.0        0.0   0.0    5.0     0.0       0.0        0.0   0.0   8.0   \n","12      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","13      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","14      0.0        0.0   0.0   10.0     5.0       0.0        0.0   0.0   9.0   \n","15      0.0        0.0   0.0   10.0     0.0       0.0        0.0   0.0   9.0   \n","16      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","17      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","18      0.0        0.0   0.0    9.0     8.0       0.0        0.0   0.0   9.0   \n","19      0.0        0.0   0.0    9.0     0.0       0.0        0.0   0.0   9.0   \n","20      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","21      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","22      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","23      0.0        0.0   0.0    0.0     0.0       0.0        0.0   0.0   0.0   \n","\n","    question  radio  salt  tool  understand  voice  why  young  student  total  \n","0        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      8.0    8.0  \n","1        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      9.0    9.0  \n","2        0.0    0.0   0.0   0.0         8.0   10.0  8.0    0.0      9.0   70.0  \n","3        0.0    0.0   0.0   0.0         0.0    8.0  8.0    0.0      9.0   46.0  \n","4        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","5        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","6        0.0    0.0   0.0   0.0         8.0   10.0  5.0    0.0     10.0   70.0  \n","7        0.0    0.0   0.0   0.0         0.0    8.0  5.0    0.0     10.0   45.0  \n","8        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","9        0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","10       0.0    0.0   0.0   0.0         8.0   10.0  6.0    0.0      9.0   78.0  \n","11       0.0    0.0   0.0   0.0         0.0    8.0  6.0    0.0      9.0   54.0  \n","12       0.0    0.0   0.0   0.0         0.0    0.0  7.0    0.0     10.0   22.0  \n","13       0.0    0.0   0.0   0.0         0.0    0.0  7.0    0.0     10.0   20.0  \n","14       5.0    0.0   0.0   0.0         8.0    7.0  8.0    0.0      9.0   75.0  \n","15       5.0    0.0   0.0   0.0         0.0    7.0  8.0    0.0      9.0   62.0  \n","16       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","17       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0     10.0   10.0  \n","18       3.0    0.0   0.0   0.0         8.0   10.0  8.0    0.0      9.0   78.0  \n","19       3.0    0.0   0.0   0.0         0.0    7.0  8.0    0.0      9.0   60.0  \n","20       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n","21       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n","22       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n","23       0.0    0.0   0.0   0.0         0.0    0.0  0.0    0.0      0.0    0.0  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BiGoF-1-wOCN"},"source":["Save results to file"]},{"cell_type":"code","metadata":{"id":"MJb345We7bEB"},"source":["scores_df.to_csv('/filepath/filename.csv')"],"execution_count":null,"outputs":[]}]}